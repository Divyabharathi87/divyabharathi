# -*- coding: utf-8 -*-
"""Welcome To Colab

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/notebooks/intro.ipynb
"""

pip install vaderSentiment

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import re
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from wordcloud import WordCloud
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer

# === DOWNLOAD NLTK RESOURCES ===
nltk.download('punkt')
nltk.download('stopwords')

# === SAMPLE DATASET (No need to load a file) ===
data = {
    "text": [
        "I am so happy today!",
        "This is the worst day ever.",
        "I'm scared of what will happen next.",
        "I love spending time with my friends.",
        "Everything is falling apart.",
        "I'm feeling excited and joyful!",
        "Why is this happening to me?",
        "That movie was hilarious and fun!",
        "I can't stop crying.",
        "He really made my day!"
    ],
    "emotion": [
        "joy", "anger", "fear", "joy", "sadness",
        "joy", "fear", "joy", "sadness", "joy"
    ]
}

df = pd.DataFrame(data)

# === CLEANING FUNCTION ===
def clean_text(text):
    text = text.lower()
    text = re.sub(r"http\S+|www\S+|https\S+", '', text)
    text = re.sub(r'\@w+|\#','', text)
    text = re.sub(r'[^A-Za-z\s]', '', text)
    text = re.sub(r'\s+', ' ', text).strip()
    return text

df['clean_text'] = df['text'].apply(clean_text)

# === PLOT EMOTION DISTRIBUTION ===
plt.figure(figsize=(8, 5))
sns.countplot(data=df, x='emotion', order=df['emotion'].value_counts().index, palette='pastel')
plt.title("Emotion Distribution")
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

# === WORD CLOUD FOR EACH EMOTION ===
emotions = df['emotion'].unique()
for emotion in emotions:
    text = " ".join(df[df['emotion'] == emotion]['clean_text'])
    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)
    plt.figure(figsize=(10, 4))
    plt.imshow(wordcloud, interpolation='bilinear')
    plt.axis('off')
    plt.title(f"Word Cloud for {emotion}")
    plt.show()

# === TF-IDF VECTORIZATION ===
vectorizer = TfidfVectorizer(max_features=1000)
X = vectorizer.fit_transform(df['clean_text'])
y = df['emotion']

# === TRAIN-TEST SPLIT ===
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# === TRAINING LOGISTIC REGRESSION ===
clf = LogisticRegression(max_iter=200)
clf.fit(X_train, y_train)
y_pred = clf.predict(X_test)

# === EVALUATION METRICS ===
print("\nClassification Report:\n")
print(classification_report(y_test, y_pred))

# === CONFUSION MATRIX ===
cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(6, 5))
sns.heatmap(cm, annot=True, fmt='d', xticklabels=clf.classes_, yticklabels=clf.classes_, cmap="YlGnBu")
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Confusion Matrix')
plt.show()

# === VADER SENTIMENT ANALYSIS ===
analyzer = SentimentIntensityAnalyzer()
df['vader_score'] = df['clean_text'].apply(lambda x: analyzer.polarity_scores(x)['compound'])

def vader_sentiment(score):
    if score >= 0.05:
        return 'positive'
    elif score <= -0.05:
        return 'negative'
    else:
        return 'neutral'

df['vader_sentiment'] = df['vader_score'].apply(vader_sentiment)

# === SAMPLE OUTPUT OF VADER SENTIMENT ===
print("\nSample VADER Sentiment Classification:")
print(df[['text', 'vader_sentiment']])